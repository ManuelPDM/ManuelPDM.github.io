<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Multi Agent Reinforcement Learning Overcooked</title>
  <link rel="stylesheet" href="style.css">
  <!-- Minimalist font -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Helvetica:wght@400;700&display=swap" rel="stylesheet">
  <!-- Font Awesome for icons -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha512-..." crossorigin="anonymous" referrerpolicy="no-referrer" />
</head>
<body>
  <header class="site-header">
    <nav>
      <ul>
         <li><a href="index.html">Home</a></li>
         <li><a href="projects.html">Projects</a></li>
      </ul>
    </nav>
  </header>
  <main>
    <section class="project-details">
       <h1>Multi Agent Reinforcement Learning Overcooked</h1>

       <p>
         <strong>Overview:</strong>
         This project explores a multi-agent deep reinforcement learning approach within the Overcooked AI environment maintained by HumanCompatibleAI. The objective was to enable two agents to learn effective cooperative strategies for maximizing soup delivery performance in varied, challenging room layouts.
       </p>

       <p>
         <strong>Approach and Tools:</strong>
         Developed using <em>PyTorch</em>, this project investigated several state-of-the-art architectures including Double Deep Q-Networks (DDQN), COMA, and MADDPG. Through extensive experimentation, a shared neural network design was adoptedâ€”leveraging CUDA for GPU acceleration to reduce training time by 50%. Robust data processing and debugging practices were employed throughout the development process.
       </p>

       <p>
         <strong>Methodology:</strong>
         The initial baseline approach was adapted from a standard DQN and then enhanced by transitioning to DDQN to address overestimation issues and improve learning stability. A novel, coordination-focused reward shaping mechanism was integrated to foster cooperative behavior between the agents. This strategy allowed both agents to effectively handle complex scenarios by dynamically adjusting rewards based on their cooperative actions.
       </p>

       <p>
         <strong>Results and Impact:</strong>
         The refined algorithm achieved stable convergence across multiple Overcooked scenarios, demonstrating robust performance in environments demanding high levels of coordination. Comprehensive model evaluation and hyperparameter optimization confirmed that the shared neural network and reward tuning significantly enhanced both learning efficiency and overall performance.
       </p>

       <p>
         <strong>Conclusion:</strong>
         This multi-agent reinforcement learning project underscores the potential of advanced deep RL techniques in cooperative settings. By integrating DDQN with precise reward shaping and leveraging GPU acceleration, the project not only improved training efficiency but also set a foundation for future research in complex multi-agent environments.
       </p>

       <!-- Figure with the downloaded GIF -->
       <figure>
         <img src="layouts.gif" alt="Demo GIF showing layouts of Overcooked AI environment" class="demo-gif">
         <figcaption>Image source: <a href="https://github.com/HumanCompatibleAI/overcooked_ai?tab=readme-ov-file" target="_blank">Overcooked AI Repository (HumanCompatibleAI)</a></figcaption>
       </figure>

       <!-- Plain-text reference -->
       <p>
         Reference: Overcooked AI Repository (HumanCompatibleAI) (<a href="https://github.com/HumanCompatibleAI/overcooked_ai?tab=readme-ov-file" target="_blank">GitHub</a>)
       </p>

       <a href="projects.html" class="back-link">Back to Projects</a>
    </section>
  </main>
  <footer>
    <p>&copy; 2025 ManuelPDM</p>
  </footer>
  <script src="script.js"></script>
</body>
</html>
